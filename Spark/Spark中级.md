>è¿ç€æŠŠè¿™ä¸€å—æ”¹æ€»ç»“çš„ç¬”è®°éƒ½å†™äº†ï¼Œæ—¶é—´å¤ªç´§å¼ äº†ï¼Œå¯¹ä¸ä½å„ä½äº†ï¼ğŸ˜‚ğŸ˜‚ğŸ˜‚

## Spark RDD

> å¼¹æ€§çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªJavaç±»ï¼Œé‡Œé¢æ”¾çš„éƒ½æ˜¯æ•°æ®ã€‚RDDä»£è¡¨ä¸€ä¸ªä¸å¯å˜çš„å¯¹å…ƒç´ åˆ†åŒºçš„é›†åˆã€‚å¹¶ä¸”RDDå¯ä»¥è¢«å¹¶è¡Œè®¡ç®—ã€‚

![](https://i.imgur.com/ENizg3K.png)
**Spark RDDç‰¹æ€§**

- åˆ†ä¸ºè‹¥å¹²ä¸ªåŒº
- æ¯ä¸ªåˆ†ç‰‡ç”¨ä¸€ä¸ªå‡½æ•°è®¡ç®—
- RDDç›´æ¥æ˜¯ä¸€ä¸ªä¾èµ–å…³ç³»
- å¯¹äºK-Vçš„RDDå¯æŒ‡å®šä¸€ä¸ªåˆ†åŒºï¼Œå‘Šè¯‰å®ƒå¦‚ä½•åˆ†ç‰‡
- è¦è¿è¡Œçš„è®¡ç®—/æ‰§è¡Œæœ€å¥½åœ¨å“ªå‡ ä¸ªæœºå™¨ä¸Šè¿è¡Œ

RDD lineage æœ‰ç”Ÿå‘½çº¿ ï¼Œå®ƒä¿å­˜äº†å¦‚ä½•è½¬æ¢å¾—æ¥çš„ä¿¡æ¯ã€‚

å¤„ç†RDD splitè¿›è¡Œè®¡ç®—æ—¶ï¼Œsplitæ•°æ®åœ¨å“ªé‡Œï¼Œæˆ‘ä»¬å°½é‡åœ¨é‚£å°æœºå™¨ä¸Šè¿›è¡Œè®¡ç®—ï¼Œç§»åŠ¨è®¡ç®—ï¼Œä¸æ˜¯ç§»åŠ¨æ•°æ®ã€‚

**RDD æ“ä½œ**

RDDçš„åˆ›å»º

- Parallelized Collections

![](https://i.imgur.com/I5GUQCV.png)

- External Datasets

![](https://i.imgur.com/NdTfMHy.png)

![](https://i.imgur.com/qnczj4V.png)

**Transformation**
![](https://i.imgur.com/VS0bIBC.png)
**Action**
![](https://i.imgur.com/aeOm4Ep.png)

**RDDçš„ä¾èµ–**

![](https://i.imgur.com/kRJecyR.png)

- çª„ä¾èµ–



![](https://i.imgur.com/jn8ZvCZ.png)

- å®½ä¾èµ–



![](https://i.imgur.com/35PrWCh.png)

**RDD Shuffle**



![](https://i.imgur.com/bA8Zkv7.png)

> The shuffle is Sparkâ€™s mechanism for re-distributing data 

é‚£äº›æ“ä½œä¼šå¼•èµ·Shuffleï¼Ÿ

- å…·æœ‰é‡æ–°è°ƒæ•´åˆ†åŒºæ“ä½œ ï¼Œegï¼šrepartition,coalesce
- *BeyKey eg: groupByKey ,reduceByKey
- å…³è”æ“ä½œ egï¼š join ï¼Œcogroup

**Spark å†…æ ¸**

![](https://i.imgur.com/ZW9eQPo.png)

![](https://i.imgur.com/4uuaw8J.png)

**DAGè°ƒåº¦**

> é˜¶æ®µçš„åˆ’åˆ†æ˜¯æŒ‰æ˜¯å¦æœ‰shuffleæ¥åˆ¤æ–­çš„ã€‚

![](https://i.imgur.com/zrqKJBB.png)

- æ¥æ”¶ç”¨æˆ·æäº¤çš„job
- æ„å»º Stageï¼Œè®°å½•å“ªä¸ª RDD æˆ–è€… Stage è¾“å‡ºè¢«ç‰©åŒ–
- é‡æ–°æäº¤ shuffle è¾“å‡ºä¸¢å¤±çš„ stage
- å°† Taskset ä¼ ç»™åº•å±‚è°ƒåº¦å™¨

**Task è°ƒåº¦**

![](https://i.imgur.com/zKQzO6I.png)

- æäº¤ taskset( ä¸€ç»„ task) åˆ°é›†ç¾¤è¿è¡Œå¹¶ç›‘æ§
- ä¸ºæ¯ä¸€ä¸ª TaskSet æ„å»ºä¸€ä¸ª TaskSetManager å®ä¾‹ç®¡ç†è¿™ä¸ª TaskSet
  çš„ç”Ÿå‘½å‘¨æœŸ
- æ•°æ®æœ¬åœ°æ€§å†³å®šæ¯ä¸ª Task æœ€ä½³ä½ç½® (process-local, node-local, 
  rack-local and then any) 
- æ¨æµ‹æ‰§è¡Œï¼Œç¢°åˆ° straggle ä»»åŠ¡éœ€è¦æ”¾åˆ°åˆ«çš„èŠ‚ç‚¹ä¸Šé‡è¯•å‡ºç° shuffle 
  è¾“å‡º lost è¦æŠ¥å‘Š fetch failed é”™è¯¯

**Partitionå’ŒTask**

![](https://i.imgur.com/A8UrtK9.png)

- Taskæ˜¯Executorä¸­çš„æ‰§è¡Œå•å…ƒ
- Taskå¤„ç†æ•°æ®å¸¸è§çš„ä¸¤ä¸ªæ¥æºï¼šå¤–éƒ¨å­˜å‚¨ä»¥åŠshuffleæ•°æ®
- Taskå¯ä»¥è¿è¡Œåœ¨é›†ç¾¤ä¸­çš„ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹ä¸Š
- ä¸ºäº†å®¹é”™ï¼Œä¼šå°†shuffleè¾“å‡ºå†™åˆ°ç£ç›˜æˆ–è€…å†…å­˜ä¸­

**æ¡ˆä¾‹**

- æ’åº

- Top key

  ## WordCount

  ```
  val rdd = sc.textFile("hdfs://master:8020/user/mapreduce/wordcount/input/wc.input")
  val wordcount = rdd.flatMap(_.split(" ")).map((_,1)).reduceByKey(_ + _)
  wordcount.collect
  wordcount.saveAsTextFile("hdfs://master:8020/user/mapreduce/wordcount/sparkOutput99")
  
  ======result
  (hive,2)
  (mapreduce,1)
  (mapreduce2,1)
  (hadoop,5)
  (hdfs,1)
  å‘ç°ï¼š
  	Spark è¿è¡ŒWordCounç¨‹åºï¼Œå¹¶æ²¡æœ‰åƒMapReduceç¨‹åºé‚£æ ·ï¼Œå¯¹Keyè¿›è¡Œæ’åºã€‚
  ## Key Sort
  wordcount.sortByKey().collect    ## é»˜è®¤æƒ…å†µæ˜¯ å‡åº
  wordcount.sortByKey(true).collect	
  wordcount.sortByKey(false).collect		
  	
  ===============================================================	
  ------result
  (hive,2)
  (mapreduce,1)
  (mapreduce2,1)
  (hadoop,5)
  (hdfs,1)	
  éœ€æ±‚ï¼š
  	æŒ‰ç…§valueå€¼è¿›è¡Œé™åº
  ## Value Sort
  wordcount.map(x => (x._2,x._1)).sortByKey(false).collect
  wordcount.map(x => (x._2,x._1)).sortByKey(false).map(x => (x._2,x._1)).collect
  	
  ## Top N
  wordcount.map(x => (x._2,x._1)).sortByKey(false).map(x => (x._2,x._1)).take(3)
  ```
----
## æ€»ç»“

å†æ°´ä¸€æ–‡ã€‚æˆ‘æ‰¿è®¤sparkæ˜¯å­¦çš„çœŸçš„èœï¼