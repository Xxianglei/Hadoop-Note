>è¿™å‡ å¤©ï¼Œå› ä¸ºå­¦é™¢çš„è€ƒå‹¤ç³»ç»Ÿéœ€è¦ç»´æŠ¤æ­å»ºï¼Œè€½è¯¯äº†æˆ‘ä¸å°‘æ—¶é—´ã€‚å¯¹äºSparkçš„å­¦ä¹ æœ¬æ¥å°±å¾ˆè–„ï¼Œè¿™æ¬¡åŸºæœ¬ä¸Šæ˜¯çœŸæ­£çš„åˆçª¥äº†ï¼Œå†™çš„å¾ˆè‰ç‡å§ã€‚åªæ˜¯ä¸€äº›å­¦ä¹ ç¬”è®°ã€‚çƒ¦å•Šï¼ğŸ˜¡ è¿™å—å‘å•¥æ—¶å€™èƒ½å¡«ä¸Šå•Šã€‚
 
## Sparkæ¦‚è¿°

**Sparkæ˜¯ä»€ä¹ˆï¼Ÿ**

> Sparkæ˜¯ä¸€ä¸ªå¿«é€Ÿçš„é€šç”¨çš„é’ˆå¯¹å¤§æ•°æ®é›†çš„è®¡ç®—æ¡†æ¶
> å®ƒå¤„ç†çš„æ•°æ®æ”¾åœ¨å†…å­˜é‡Œé¢ï¼Œè®¡ç®—æ—¶é‡‡ç”¨DAGæœ‰å‘æ— ç¯å›¾ã€‚ä¸ºä»€ä¹ˆå«é€šç”¨çš„ï¼Œæ˜¯å› ä¸ºä»–å¯ä»¥åšHadoopçš„å„ç§åŠŸèƒ½æ¨¡å—ï¼Œä¸éœ€è¦å…¶ä»–æ¡†æ¶è¾…åŠ©ã€‚

**Sprakç‰¹æ€§**

![](https://i.imgur.com/iXQH0hp.png)

- è®¡ç®—æ¯”Mapreduceå¿«ä¸€ç™¾å€ã€‚
- æ•°æ®æ”¾åœ¨ç£ç›˜æ¯”MRå¿«10å€ã€‚
- Sparkç®€å•æ˜“ç”¨æ”¯æŒ R java python Scala
- é€šç”¨æ€§ç»§æ‰¿äº†SQL streaming å’Œå¤æ‚è®¡ç®—
- Sparkå¯ä»¥è¿è¡Œåœ¨ä»»ä½•åœ°æ–¹

![](http://spark.apache.org/images/spark-runs-everywhere.png)

**MapReduceä¸Sparkçš„ç±»ä¼¼æ¡†æ¶**

```
MapReduce
	Hive		Storm			Mahout		Griph

Spark Core
	Spark SQL	Spark Streaming	Spark ML	Spark GraphX	Spark R
```

SparkStreaming æ¯”storm ç”¨çš„å¤šã€‚Sparkä¸å¯ä»£æ›¿MRã€‚

MRä¸­æ•°æ®ä¼ è¾“æ˜¯kvå½¢å¼ï¼ŒSparkæ˜¯1ä»¥RDDï¼ˆå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼‰çš„å½¢å¼ã€‚

**Sparkç”Ÿæ€ç³»ç»Ÿ**

![](https://i.imgur.com/EMEj44G.png)

Hive åº•å±‚çš„å¼•æ“è¿è¡Œå¯ä»¥ç›´æ¥è½¬æ¢æˆ MapReduce Tez Spark
BlinkDB æ˜¯ä¸€ä¸ªåœ¨è¯¯å·®ç•Œé™å†…æˆ–è€…ä¸€å®šè¿”å›æ—¶é—´å†…çš„SQLæŸ¥è¯¢

**MRä¸Sparkçš„åŒºåˆ«**

![](https://i.imgur.com/d9Kw1vj.png)

![](https://i.imgur.com/XK1KARs.png)

## Sparkç¼–è¯‘ã€æµ‹è¯•

> The Maven-based build is the build of reference for Apache Spark. Building Spark using Maven requires Maven 3.5.4 and Java 8. Note that support for Java 7 was removed as of Spark 2.2.0.

**ä½ å¯ä»¥ä½¿ç”¨CDHå¯¹åº”çš„ç‰ˆæœ¬ã€‚**
- ä¸‹è½½æºç 
- [ç¼–è¯‘ç»†èŠ‚](http://spark.apache.org/docs/latest/building-spark.html)
  - SBT ç¼–è¯‘
  - Maven ç¼–è¯‘
  - æ‰“åŒ…ç¼–è¯‘make-distribution.sh

- Mavenå®‰è£…éƒ¨ç½²
  - ä¸‹è½½åœ°å€
    http://maven.apache.org/download.cgi
  - è§£å‹
    tar -zvxf apache-maven-3.0.5
  - é…ç½®ç¯å¢ƒå˜é‡
    export MAVEN_HOME=/opt/modules/maven-3.0.5
    export PATH=$PATH:$MAVEN_HOME/bin
  - éªŒè¯
    mvn -version

- è§£å‹ tar -zvxf spark-1.3

  ### MAVEN ç¼–è¯‘

  $ /opt/modules/spark-1.3.0-src/build/mvn clean package -DskipTests -Phadoop-2.4 -Dhadoop.version=2.5.0-cdh5.3.6 -Pyarn -Phive-0.13.1 -Phive-thriftserver

  ### æ‰“åŒ…ç¼–è¯‘ï¼Œä½¿ç”¨CDH 5.3.6ç‰ˆæœ¬çš„HADOOP

  ./make-distribution.sh --tgz -Phadoop-2.4 -Dhadoop.version=2.5.0-cdh5.3.6 -Pyarn -Phive-0.13.1 -Phive-thriftserver

  ### æ‰“åŒ…ç¼–è¯‘ï¼Œä½¿ç”¨Apacheç‰ˆæœ¬çš„HADOOP

  ./make-distribution.sh --tgz -Phadoop-2.4 -Dhadoop.version=2.5.0 -Pyarn -Phive-0.13.1 -Phive-thriftserver

ç¼–è¯‘sparkæ³¨æ„ï¼š1ï¼‰æ›¿æ¢maven ä»“åº“jaråŒ… 2ï¼‰é…ç½®åŸŸåè§£ææœåŠ¡ 3ï¼‰ä¿®æ”¹make-distribution.sh 4ï¼‰æ‰§è¡Œæ‰“åŒ…ç¼–è¯‘

## Sparkè¿è¡Œæ–¹å¼

> Local Standalone Yarn  Mesos

å®‰è£…scala

é…ç½®ç¯å¢ƒå˜é‡

![](https://i.imgur.com/cyFN3uB.png)

è§£å‹spark-1.3.0-bin-2.5.0-cdh5.3.6.tar.gz

ç›´æ¥å¯åŠ¨æµ‹è¯•å®˜æ–¹ä¾‹å­ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰

![](https://i.imgur.com/u6ZN5uF.png)
![](https://i.imgur.com/QulmeIr.png)

```
//ä¸€è¡Œä¸€è¡Œçš„è¯» æŠŠå€¼å°è£…åˆ°RDD
 var testFile = sc.textFile("README.md")

testFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:21

å¯ä»¥å°†å‡½æ•°Aä½œä¸ºå‚æ•°ä¼ é€’ç»™å‡½æ•°B,æ­¤æ—¶è¿™ä¸ªå‡½æ•°Bå«åšé«˜é˜¶å‡½æ•°
textFile.filter((line: String) => line.contains("Spark"))
textFile.filter(line => line.contains("Spark"))
textFile.filter(line => line.contains("Spark"))
textFile.filter(_.contains("Spark"))
ä¸‹åˆ’çº¿è¡¨ç¤ºä»»æ„å…ƒç´ 

åŒ¿åå‡½æ•°
(line: String) => line.contains("Spark")
def func01(line: String) {
	line.contains("Spark")
} 

def func01(line: String) => line.contains("Spark")
```

â€‹	

```
filter(f: T => Boolean)
(line: String) => line.contains("Spark")

T:
	(line: String)
Boolean:
	line.contains("Spark")
```

å‡½æ•°æ²¡æœ‰å‚æ•°æ—¶ï¼Œæ‹¬å·å¯ä»¥çœç•¥ã€‚

å‘ç°å‘½ä»¤è¡Œæ˜¯Scala

![](https://i.imgur.com/7XWrSWw.png)

Webç«¯å£ http://192.168.1.205:4040 å¼€å¤šä¸ªçš„æ—¶å€™ä»4040ç«¯å£æš‚ç”¨ ç«¯å£å·å¾€åå˜ã€‚

## Sparkéƒ¨ç½²

> Spark Standalone å®‰è£…éƒ¨ç½²æ¨¡å¼æŒ‡çš„æ˜¯Sparkæœ¬èº«è‡ªå¸¦çš„é›†ç¾¤ç®¡ç†ã€‚

å®ƒä¹Ÿæ˜¯åˆ†å¸ƒå¼çš„

ä¸»èŠ‚ç‚¹ Master
ä»èŠ‚ç‚¹ Work Nodes

![](https://i.imgur.com/DdF7NeG.png)
![](https://i.imgur.com/SkPTZA0.png)
éœ€è¦å®‰è£…çš„å†…å®¹

1ï¼‰JAVA

2ï¼‰SCALA

3ï¼‰HDFS

4ï¼‰SPARK

- ä¿®æ”¹slaves 

![](https://i.imgur.com/g4a5Pd9.png)

- é‡å‘½åæ—¥å¿—æ–‡ä»¶

  mv log4j.properties.template log4j.properties

- ä¿®æ”¹ç¯å¢ƒé…ç½® SPARK_WORKER_DIR=

  mv spark-env.sh.template spark-env.sh

![](https://i.imgur.com/zmThxbJ.png)

![](https://i.imgur.com/FEPa6gv.png)

- ä¿®æ”¹é…ç½® 

![](https://i.imgur.com/HwWwPFg.png)

![](https://i.imgur.com/c3fmAZe.png)

å¯ä»¥æ‰‹åŠ¨æŒ‡å®šï¼šspark-shell --master spark://master:7077

- å¯åŠ¨ start-master.sh  start-slaves.sh æˆ–è€…start-all.sh

![](https://i.imgur.com/Nb1JeU2.png)

- æµ‹è¯• 

  sc.textFile("/user/data/page_views.data")

  res2.collect

- WorkCount æµ‹è¯•

  val linesRdd = sc.textFile("hdfs://master:8020/user/beifeng/mapreduce/wordcount/input/wc.input")

  val wordsRdd = linesRdd.flatMap(line => line.split(" "))

  val keyvalRdds = wordsRdd.map(word => (word,1))

  val countRdd = keyvalRdds.reduceByKey((a,b) => (a + b))

  hdfs-file -> linesRdd  -> wordsRdd -> keyvalRdds -> countRdd -> arr

ç®€å†™
â€‹	

```
sc.textFile("hdfs://master:8020/user/beifeng/mapreduce/wordcount/input/wc.input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_ + _)
```

hdfs -> rdd  -> wordRdd  -> kvRdd  -> wordCountRdd -> hdfs

RDD lineage ç”Ÿå‘½çº¿ ï¼Œä¿å­˜å¦‚ä½•è½¬æ¢å¾—æ¥çš„

## Spark spark-submit

![](https://i.imgur.com/2So4iJk.png)

![](https://i.imgur.com/GKYFRQx.png)

---
## æ€»ç»“

ä¸åšä¿®æ”¹äº†å°±è¿™æ ·å§ï¼å› ä¸ºæ²¡å­¦Scala æ‰€ä»¥Sparkå­¦çš„å¤ªåƒåœ¾äº†ï¼å†æ‰¾æ—¶é—´è¡¥å§ï¼