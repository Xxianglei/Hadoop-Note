>æ…Œçš„ä¸€æ‰¹ã€‚ğŸ˜ğŸ˜ğŸ˜



## Spark HistoryServer



> åº”ç”¨åœ¨è¿è¡Œæ—¶å€™å¯ä»¥ç”¨4040ç«¯å£æä¾›WEBç›‘æ§ã€‚é‚£å½“åº”ç”¨å®Œæˆæˆ–è€…è¯´è¿è¡Œå¤±è´¥çš„æ—¶å€™æ€ä¹ˆå»ç›‘æ§å‘¢ï¼Ÿé‚£å°±æ˜¯HistoryServerã€‚



![](https://i.imgur.com/JNu6o7H.png)



**é…ç½®**



> é…ç½®åœ¨spark-env.shä¸­çš„SPARK_HISTORY_OPTS,å¯ä»¥ç†è§£ä¸ºæœåŠ¡ç«¯çš„é…ç½®



![](https://i.imgur.com/vV4AdoC.png)



> é…ç½®åœ¨spark-defaults.conf,å¯ä»¥ç†è§£ä¸ºå®¢æˆ·ç«¯çš„é…ç½®



![](https://i.imgur.com/lctUGYj.png)

- å¯åŠ¨ HistoryServer

  > ./start-history-server.sh

## Spark on Yarn

> the driver program è¿è¡Œåœ¨ä»€ä¹ˆåœ°æ–¹ï¼Ÿ
>
> > æœ¬åœ°ï¼Œåº”ç”¨è¿è¡Œçš„è¿™å°æœºå™¨ï¼Œå¦‚å›¾ä¸€
> >
> > work èŠ‚ç‚¹çš„æŸå°æœºå™¨ä¸Šï¼Œå¦‚å›¾äºŒ

![](https://i.imgur.com/u4yAJ27.png)


![](https://i.imgur.com/egWiBEQ.png)

**Shell å‘½ä»¤**

```shell
/spark-submit \
--master spark://master:7077 \
--deploy-mode client \
jars / sparkAPP.jar 
```



```shell
/spark-submit \
--master spark://master:7077 \
--deploy-mode cluster \
jars / sparkAPP.jar
```

------

#### Spark on Yarnæ¶æ„å›¾

![](https://i.imgur.com/0QZdKVS.png)

> å®¢æˆ·ç«¯æäº¤ç»™RM	
>
> RM-->AM(Spark)
>
> Spark APP --> Executors
>
> > SAM->RM ç”³è¯·è¿è¡Œæ‰€éœ€è¦çš„èµ„æº
>
> Driver program 
>
> > DAG->DAGScheduler->StageScheduler->TaskSet


![](https://i.imgur.com/eiUibZN.png)

**Shell å‘½ä»¤**

é¦–å…ˆå¯åŠ¨Yarn

å°†spark-shellè¿è¡Œåœ¨yarnä¸Šï¼Œä»¥client æ¨¡å¼

```shell
./bin/spark-shell --master yarn --deploy-mode client
```
![](https://i.imgur.com/wyObzbE.png)

```shell
./bin/spark-submit  \
    --master yarn-cluster \
    jars/sparkApp.jar
```

## Spark Streaming



> Streamingï¼šæ˜¯ä¸€ç§æ•°æ®ä¼ é€æŠ€æœ¯ï¼Œå®ƒæŠŠå®¢æˆ·æœºæ”¶åˆ°çš„æ•°æ®å˜æˆä¸€ä¸ªç¨³å®šè¿ç»­çš„
> æµï¼Œæºæºä¸æ–­åœ°é€å‡ºï¼Œä½¿ç”¨æˆ·å¬åˆ°çš„å£°éŸ³æˆ–çœ‹åˆ°çš„å›¾è±¡ååˆ†å¹³ç¨³ï¼Œè€Œä¸”ç”¨æˆ·åœ¨
> æ•´ä¸ªæ–‡ä»¶é€å®Œä¹‹å‰å°±å¯ä»¥å¼€å§‹åœ¨å±å¹•ä¸Šæµè§ˆæ–‡ä»¶ã€‚

æµå¼è®¡ç®—

- Apache Strom
- Spark Streaming
- Apache Samza

ä¸Šè¿°ä¸‰ç§å®æ—¶è®¡ç®—ç³»ç»Ÿéƒ½æ˜¯å¼€æºçš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œå…·æœ‰ä½å»¶è¿Ÿã€å¯æ‰©å±•å’Œå®¹é”™æ€§
è¯¸å¤šä¼˜ç‚¹ï¼Œå®ƒä»¬çš„å…±åŒç‰¹è‰²åœ¨äºï¼šå…è®¸ä½ åœ¨è¿è¡Œæ•°æ®æµä»£ç æ—¶ï¼Œå°†ä»»åŠ¡åˆ†é…åˆ°
ä¸€ç³»åˆ—å…·æœ‰å®¹é”™èƒ½åŠ›çš„è®¡ç®—æœºä¸Šå¹¶è¡Œè¿è¡Œã€‚æ­¤å¤–ï¼Œå®ƒä»¬éƒ½æä¾›äº†ç®€å•çš„APIæ¥
ç®€åŒ–åº•å±‚å®ç°çš„å¤æ‚ç¨‹åº¦ã€‚



**Spark Streaming æ˜¯ä»€ä¹ˆ**





> Spark Streaming is an extension(æ‰©å±•) of the core Spark API that enables scalableï¼ˆå¯æ‰©å±•æ€§ï¼‰, high-throughputï¼ˆé«˜ååé‡ï¼‰, fault-tolerant stream processing of live data streamsï¼ˆæµå¼å¤„ç†ï¼‰.


![](https://i.imgur.com/LWWtQdi.png)



> åŸå§‹æ•°æ®æµ->åˆ†å¸ƒå¼æµå¼è®¡ç®—ç³»ç»Ÿ->è¾“å‡ºæ•°æ®


![](https://i.imgur.com/lTchvs6.png)

**å®˜æ–¹ä¾‹å­**

- ä»Socketå®æ—¶è¯»å–æ•°æ®ï¼Œè¿›è¡Œå®æ—¶å¤„ç†
- rpm -ivh nc-1.84-22.el6.x86_64.rpm
- è¿è¡Œncé’ˆå¯¹äºç«¯å£å·9999
  $ nc -lk 9999
- è¿è¡ŒDemo
  bin/run-example streaming.NetworkWordCount master 9999

![](https://i.imgur.com/kVHgVUp.png)



**ä»£ç ä¾‹å­ï¼š**


![](https://i.imgur.com/TIeZn2M.png)

## Spark Streamingæ˜¯å¦‚ä½•å·¥ä½œçš„

- æµæ•°æ®åˆ†æ‰¹ï¼Œåº•å±‚å®è´¨ä¸Šè¿˜æ˜¯ä¸€ä¸ªæ‰¹å¤„ç†ã€‚æ¯ä¸€ä¸ªbatchäº¤ç»™coreå¤„ç†ã€‚

![](https://i.imgur.com/AIlLuOx.png)

- äºŒ

![](https://i.imgur.com/ghtRHVW.png)

![](https://i.imgur.com/1nbEhca.png)

![](https://i.imgur.com/VsaA0EV.png)

![](https://i.imgur.com/Kx7lGKg.png)


## Spark Streamingç¼–ç¨‹



- æ–¹å¼ä¸€ï¼šIDEç¼–ç¨‹ç”¨çš„æ¯”è¾ƒå¤š

![](https://i.imgur.com/NJxvSOJ.png)

- æ–¹å¼äºŒï¼šspark shellç”¨çš„æ¯”è¾ƒå¤š

![](https://i.imgur.com/Zt5HAxO.png)

åŸºæœ¬æ­¥éª¤ï¼š

- é€šè¿‡åˆ›å»ºä¸€ä¸ªÂ·Dstreamå®šä¹‰ä¸€ä¸ªæ•°æ®æµ
- å®šä¹‰æµå¼è®¡ç®—ï¼Œè¿›è¡Œä¸€äº›è½¬æ¢å’Œè¾“å‡º
- å¼€å§‹æ¥æ”¶æ•°æ®å¹¶ä¸”å¤„ç†å®ƒ   streamingContexã€‚start()
- ç­‰å¾…åº”ç”¨å¤„ç†çš„ä»‹ç»    streamingContex.awaitTermination()
- å¯ä»¥æ‰‹åŠ¨åœæ­¢    streamingContext.stopï¼ˆï¼‰

**ä»£ç æ¨¡æ¿**

```scala
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._ 

val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")
val ssc = new StreamingContext(conf, Seconds(5))

// è¯»å–æ•°æ® å®è´¨åˆ›å»ºäº†ä¸€ä¸ªDstream
val lines = ssc.socketTextStream("localhost", 9999)

// æ•°æ®å¤„ç†
val words = lines.flatMap(_.split(" "))
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.print()

//å¯åŠ¨åº”ç”¨
ssc.start()             // å¼€å§‹è®¡ç®—
ssc.awaitTermination()  // ä¸€ç›´ç­‰åˆ°è®¡ç®—ç»ˆæ­¢

```



**HDFSä¸Šçš„æ•°æ®æ“ä½œ**



```scala
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._ 

val ssc = new StreamingContext(sc, Seconds(5))

// ä»HDFSä¸Šé¢è¯»å–æ–‡æœ¬æ•°æ®
val lines = ssc.textFileStream("hdfs://master:8020/user/streaming/input/hdfs/")

// æ•°æ®å¤„ç†
val words = lines.flatMap(_.split("\t"))
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.print()
 
ssc.start()             // å¼€å§‹è®¡ç®—
ssc.awaitTermination()  //  ä¸€ç›´ç­‰åˆ°è®¡ç®—ç»ˆæ­¢
```

æµ‹è¯•ï¼š

- åˆ›å»º wc.txt
- åˆ›å»ºæ–‡ä»¶ç›®å½•ï¼š./hdfs dfs -mkdir -p /user/streaming/input/hdfs ï¼ˆç›‘æ§çš„æ–‡ä»¶å¤¹ï¼‰
- ä¸Šä¼ æ–‡ä»¶ ï¼š./hdfs dfs -put /opt/datas/wc.txt /user/streaming/input/hdfs
- å¼€å¯ ./spark-shell --master local[2]
- æ‹·è´ä»£ç è¿è¡Œ



```scala
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._ 

val ssc = new StreamingContext(sc, Seconds(5))

// read data
val lines = ssc.textFileStream("hdfs://master:8020/user/streaming/input/hdfs/")

// process
val words = lines.flatMap(_.split(" "))
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.saveAsTextFiles("hdfs://master:8020/user/streaming/output/")
 
ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate
```



å¦‚ä½•åœ¨Spark-shellä¸­æ‰§è¡ŒæŸä¸ªscalaä»£ç ï¼Œå¿«é€Ÿå¼€å‘å¯ä»¥åˆ›å»ºä¸€ä¸ªxx.scalaçš„æ–‡ä»¶ 

æ‰§è¡Œ      :load /opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/HDFSWordCount.scala

## Spark Streamingé›†æˆFlume

> Flumeæœ‰ä¸‰ä¸ªç»„ä»¶
> â€‹	Source  --->   Channel  --->   Sink(Spark Streaming) ä½œä¸ºæ¥å—è€…ï¼Œæ¥å—æ•°æ®è¿›è¡Œå¤„ç†ã€‚



å¯¼å…¥å¿…è¦çš„åŒ…ï¼š

![](https://i.imgur.com/kEaAgK7.png)
ä»£ç ï¼š

```scala
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._ 
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel

val ssc = new StreamingContext(sc, Seconds(5))

// read data
val stream = FlumeUtils.createStream(ssc, "master", 9999, StorageLevel.MEMORY_ONLY_SER_2)

stream.count().map(cnt => "Received " + cnt + " flume events." ).print()

ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate
```

è¿è¡Œshell åŠ è½½ä¾èµ–åŒ…

```shell
./spark-shell --jars \
/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/spark-streaming-flume_2.10-1.3.0.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/flume-avro-source-1.5.0-cdh5.3.6.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/flume-ng-sdk-1.5.0-cdh5.3.6.jar

```

å¼€å¯flume

```shell
bin/flume-ng agent -c conf -n a2 -f conf/flume-spark-push.sh -Dflume.root.logger=DEBUG,console
```

## Spark Streaming é›†æˆKafkaé›†ç¾¤

![](https://i.imgur.com/eeIDr2y.png)

**Kafkaæ¶æ„**

![](https://i.imgur.com/0Ijjk7E.png)

- ç”Ÿäº§è€…ï¼šæ˜¯èƒ½å¤Ÿå‘å¸ƒæ¶ˆæ¯åˆ°è¯é¢˜çš„ä»»ä½•å¯¹è±¡
- Topicï¼šæ˜¯ç‰¹å®šç±»å‹çš„æ¶ˆæ¯æµã€‚æ¶ˆæ¯æ˜¯å­—èŠ‚çš„æœ‰æ•ˆè´Ÿè½½ï¼Œè¯é¢˜æ˜¯æ¶ˆæ¯çš„åˆ†ç±»åæˆ–ç§å­å
- æ¶ˆè´¹è€…ï¼šå¯ä»¥è®¢é˜…ä¸€ä¸ªæˆ–è€…å¤šä¸ªTopicå¹¶ä¸”ä»Brokeræ‹‰æ•°æ®
- ä»£ç†ï¼ˆKafkaé›†ç¾¤ï¼‰ï¼šå·²å‘å¸ƒçš„æ¶ˆæ¯ä¿å­˜åœ¨ä¸€ç»„æœåŠ¡å™¨ä¸­

## Kafka å®‰è£…é…ç½®

åŸºæœ¬ç¯å¢ƒï¼š

![](https://i.imgur.com/JcLBIKY.png)

- è§£å‹ tar -zxvf kafka_2.10-0.8.2.1.tgz -C /opt/cdh-5.3.6/
- æ›´æ¢ libä¸‹çš„ zookeeper jaråŒ…ä¸ºè‡ªå·±åˆé€‚çš„ zookeeper jar 


![](https://i.imgur.com/hp7CPzP.png)

- ä¿®æ”¹é…ç½®æ–‡ä»¶ vi server.properties

  ```xml
  ############################# Server Basics #############################
  
  # The id of the broker. This must be set to a unique integer for each broker.
  broker.id=0
  
  
  # The port the socket server listens on
  port=9092
  
  host.name=master
  
  # java.net.InetAddress.getCanonicalHostName().
  #advertised.host.name=<hostname routable by clients>
  
  # The number of threads handling network requests
  num.network.threads=3
  
  # The number of threads doing disk I/O
  
  socket.request.max.bytes=104857600
  # A comma seperated list of directories under which to store log files
  log.dirs=/opt/cdh-5.3.6/kafka_2.10-0.8.2.1/tmp/kafka-logs
  
  # parallelism for consumption, but this will also result in more files across
  # the brokers.
  num.partitions=1
  # There are a few important trade-offs here:
  
  # The number of messages to accept before forcing a flush of data to disk
  #log.flush.interval.messages=10000
  
  
  # from the end of the log.
  
  # The minimum age of a log file to be eligible for deletion
  log.retention.hours=168
  
  # A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
  # segments don't drop below log.retention.bytes.
  #log.retention.bytes=1073741824
  
  # The maximum size of a log segment file. When this size is reached a new log segment will be created.
  log.segment.bytes=1073741824
  
  # The interval at which log segments are checked to see if they can be deleted according
  # to the retention policies
  log.retention.check.interval.ms=300000
  
  # By default the log cleaner is disabled and the log retention policy will default to just delete segments after their retention expires.
  # If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.
  log.cleaner.enable=false
  
  ############################# Zookeeper #############################
  
  # Zookeeper connection string (see zookeeper docs for details).
  # This is a comma separated host:port pairs, each corresponding to a zk
  # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
  # You can also append an optional chroot string to the urls to specify the
  # root directory for all kafka znodes.
  zookeeper.connect=master:2181
  
  # Timeout in ms for connecting to zookeeper
  zookeeper.connection.timeout.ms=6000
  ```



- å¯åŠ¨Kafka

 ./kafka-server-start.sh  ../config /server.properties



**ç®€å•å®ç”¨**

å¯åŠ¨Kafkaé›†ç¾¤ æ”¾åˆ°åå°å»å¯åŠ¨
â€‹	nohup bin/kafka-server-start.sh config/server.properties & 

åˆ›å»ºtopicå‘½ä»¤
â€‹	./kafka-topics.sh --create --zookeeper master:2181 --replication-factor 1 --partitions 1 --topic test

æŸ¥çœ‹å·²ç”¨topic
â€‹	./kafka-topics.sh --list --zookeeper master:2181

ç”Ÿäº§æ•°æ®

ä¿®æ”¹é…ç½® producer.properties

![](https://i.imgur.com/5iAmjll.png)

â€‹	./kafka-console-producer.sh --broker-list master:9092 --topic test

æ¶ˆè´¹æ•°æ®
â€‹	./kafka-console-consumer.sh --zookeeper master:2181 --topic test --from-beginning

![](https://i.imgur.com/gF7uiCz.png)

![](https://i.imgur.com/2E0Olzq.png)



**Spark Streamingä¸kafkaé›†æˆç»†èŠ‚**



```scala
===============Spark Streaming + Kafka Integration åŸºäºæ¥æ”¶è€…=================
import java.util.HashMap
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._ 
import org.apache.spark.streaming.kafka._

val ssc = new StreamingContext(sc, Seconds(5))

val topicMap = Map("test" -> 1)

// read data
val lines = KafkaUtils.createStream(ssc, "master:2181", "testWordCountGroup", topicMap).map(_._2)

val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()

ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate

------------------
bin/spark-shell --jars \
/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/spark-streaming-kafka_2.10-1.3.0.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/kafka_2.10-0.8.2.1.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/kafka-clients-0.8.2.1.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/zkclient-0.3.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/metrics-core-2.2.0.jar

ç¬¬ä¸¤ç§æ–¹å¼ åŸºäº
===============Spark Streaming + Kafka Integration=================
import kafka.serializer.StringDecoder
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._ 
import org.apache.spark.streaming.kafka._

val ssc = new StreamingContext(sc, Seconds(5))

val kafkaParams = Map[String, String]("metadata.broker.list" -> "master:9092")
val topicsSet = Set("test")

// read data
val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)

val lines = messages.map(_._2)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()

ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate

------------------
bin/spark-shell --jars \
/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/spark-streaming-kafka_2.10-1.3.0.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/kafka_2.10-0.8.2.1.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/kafka-clients-0.8.2.1.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/zkclient-0.3.jar,/opt/cdh-5.3.6/spark-1.3.0-bin-2.5.0-cdh5.3.6/externallibs/metrics-core-2.2.0.jar
```



----
## æ€»ç»“
èœçš„ä¸€é€¼ã€‚ğŸ˜‡  --> é¦–å°¾å‘¼åº”ï¼å¦™å“‰ï¼
