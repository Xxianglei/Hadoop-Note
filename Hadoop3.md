>è¯´å®Œè¿™ä¸€è®²ï¼ŒHadoopå››ä¸ªæ ¸å¿ƒæ¨¡å—çš„å†…å®¹åŸºæœ¬ä¸Šå°±ç»“æŸäº†ã€‚å‰é¢è®²è¿‡äº†åŸºç¡€çš„éƒ¨ç½²ï¼ŒåŒ…æ‹¬å•æœºã€ä¼ªåˆ†å¸ƒå¼ï¼Œè™½ç„¶å®Œå…¨åˆ†å¸ƒå¼å…¶å®ä¹ŸæŒºç®€å•çš„ï¼Œä½†æ˜¯æ—¢ç„¶æ˜¯çŸ¥è¯†çš„æ¢³ç†ï¼Œåœ¨æœ¬èŠ‚æˆ‘ä¹Ÿåšä¸ªè®²è§£å§ã€‚æœ¬èŠ‚æœ€é‡è¦çš„å†…å®¹æ˜¯å¯¹HDFSçš„HAæ¶æ„çš„æ­å»ºã€‚ä¸€å¹´å‰çœ‹å¾—æˆ‘å¤´å¤§ï¼Œå…¶å®å˜›æ²¡æœ‰é‚£ä¹ˆéš¾ï¼Œåªæ˜¯è¢«é«˜ç«¯å¤§æ°”çš„åå­—ç»™å“ç€äº†ã€‚ğŸ˜

## å…ˆè°ˆå®Œå…¨åˆ†å¸ƒå¼æ¶æ„
---

>å®Œå…¨åˆ†å¸ƒå¼çš„æ¦‚å¿µå¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯æŠŠNamenodeï¼ŒResourceManagerï¼ŒDatanodeç­‰ï¼Œåˆ†å¼€æ”¾ç½®åˆ°ä¸åŒçš„æœºå™¨è¿è¡Œï¼Œåˆ†æ‹…ä¸»èŠ‚ç‚¹çš„å‹åŠ›ï¼Œä»¥è¾¾åˆ°æå‡é›†ç¾¤çš„å¤„ç†èƒ½åŠ›ã€‚ç™¾åº¦çš„ä¸€å¼ æ¶æ„å›¾ï¼ŒåŸºæœ¬ä¸Šå°±è¿™æ„æ€ã€‚ 
>
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20181114201134640.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1ODMzMTY=,size_16,color_FFFFFF,t_70)
é‡ç‚¹æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ç€æ‰‹æ­å»ºä¸€ä¸ªåˆ†å¸ƒå¼é›†ç¾¤ã€‚åœ¨Hadoopåˆçª¥é‡Œé¢æˆ‘æè¿‡ä¸€å˜´ï¼ˆå¤šæå‡ å°æœºå™¨ ä¿®æ”¹DataNodeä¸ªæ•° é…ç½®slaveï¼ˆæŒ‡å®šDataNodeï¼‰æ–‡ä»¶ é…ç½®hostsæ–‡ä»¶ ssh OK OKä¸ï¼Ÿ åæ­£æˆ‘ä¸æƒ³æ‰“å­—å†™åˆ†å¸ƒå¼é…ç½®äº†ã€‚ï¼‰åˆ«çœ‹æˆ‘è¯´çš„å€’æ˜¯è½»æ¾ï¼Œç†æ¸…äº†æ€è·¯ï¼ŒæŒ‰ç€è·¯å­æ­å…¶å®çš„ç¡®ä¹Ÿè½»æ¾ã€‚åé¢æˆ‘ä¼šç”¨Cloudera Manageråœ¨æˆ‘å±‹ä¸‰å°æœåŠ¡å™¨ä¸Šæ­å»ºä¸€ä¸ªä¼ä¸šçº§ğŸ™„çš„é›†ç¾¤ã€‚é‚£ä¸ªå¯æ¯”æ‰‹åŠ¨æ­å»ºèˆ’æœå¤ªå¤šäº†ã€‚å…·ä½“ç»†èŠ‚æˆ‘åé¢ä¹Ÿä¼šåšä¸ªä»‹ç»ã€‚

**ç¡¬ä»¶é…ç½®**
|  192.168.1.205|192.168.1.206  |192.168.1.207 |
|-|--|--|
|  master|  slave1|slave2 |
|  1.5G|  1G|1G |
|  1 CPU|  1 CPU|1 CPU |
**é›†ç¾¤è§„åˆ’**
HDFS:
|  192.168.1.205|192.168.1.206  |192.168.1.207 |
|--|--|--|
|  master|  slave1|slave2 |
| NameNode |DataNode  | DataNode|
| DataNode|  |SecondaryNameNode |
Yarn:
|  192.168.1.205|192.168.1.206  |192.168.1.207 |
|--|--|--|
|  master|  slave1|slave2 |
| NodeManager |NodeManager | NodeManager |
| |  |ResourceManager|
**æ­å»ºæ€è·¯ï¼š**
* ç¬¬ä¸€æ­¥ï¼šæœºå™¨å‡†å¤‡
	* å…‹éš†æœºå™¨
	* ä¿®æ”¹ip
	* ä¿®æ”¹ä¸»æœºå 
	* è®¾ç½®å¥½æ˜ å°„ 

* ç¬¬äºŒæ­¥ï¼šæ–‡ä»¶é…ç½®
	* HDFS 
	     * hadoop-env.sh  é…ç½®ï¼šjava_home å°±å¹²è¿™äº‹
		 * core-site.xml  ä¼ªåˆ†å¸ƒå¼åŸºç¡€ä¸Šä¸åšä»€ä¹ˆæ”¹å˜
		 * hdfs-site.xml
			![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20181114203453197.png)ï¼ˆé»˜è®¤å°±æ˜¯ä¸‰ä¸ªå¯ä»¥åˆ æ‰ï¼‰
			
			![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20181114203507301.png)
		 * slaves
          ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20181114203557360.png)
	* YARN
		 * yarn-env.sh  é…ç½®ï¼šjava_home å°±å¹²è¿™äº‹
		 * yarn-site.xml
	      ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2018111420364920.png)
	* MapReduce
		 * mapred-env.sh  é…ç½®ï¼šjava_home å°±å¹²è¿™äº‹
		 * mapred-site.xml	  ä¼ªåˆ†å¸ƒå¼åŸºç¡€ä¸Šä¸åšä»€ä¹ˆæ”¹å˜
* ç¬¬ä¸‰æ­¥ï¼šSSHæ— å¯†ç ç™»å½• 
	* ssh-keygen -t rsa
	* ssh-copy-id èŠ‚ç‚¹
	* æµ‹è¯•
	* ssh èŠ‚ç‚¹
	* exit
* ç¬¬å››æ­¥ï¼šæ–‡ä»¶åŒæ­¥
* scp -r xxx slave1:xxx/xxx
 
* ç¬¬äº”æ­¥:é…ç½®æ—¶é—´åŒæ­¥ 
	* ntpçš„é…ç½®æˆ‘æ²¡é…ï¼Œæˆ‘å­¦ä¹ ç¯å¢ƒå°±æ²¡å¼„è¿™ä¸ªï¼Œå¦‚æœçœŸå®çš„å¼€å‘ä¸€å®šè¦é…ç½®ã€‚

**é›†ç¾¤æµ‹è¯•**
* åŸºæœ¬æµ‹è¯• 

		1. æœåŠ¡å¯åŠ¨ï¼Œæ˜¯å¦å¯ç”¨ï¼Œç®€å•çš„åº”ç”¨
		2. HDFSåˆ›å»ºå’Œåˆ é™¤æ˜¯å¦èƒ½å¤ŸæˆåŠŸ
		3. yarn run jar
		4. mapreduce

* åŸºå‡†æµ‹è¯• 
		
			1. æµ‹è¯•é›†ç¾¤çš„æ€§èƒ½
			2. hdfs:å†™æ•°æ®ã€è¯»æ•°æ®
	    
* ç›‘æ§é›†ç¾¤ 
	
			1.Cloudera Manager
				éƒ¨ç½²å®‰è£…é›†ç¾¤
				ç›‘æ§é›†ç¾¤
				é…ç½®åŒæ­¥é›†ç¾¤
				é¢„è­¦

## å†è°ˆå®ç°HA
---
>å…ˆå¼•å‡ºä¸€ä¸ªåˆ†å¸ƒå¼æœåŠ¡æ¡†æ¶Zookeeperã€‚
>>ZooKeeperæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼çš„ï¼Œå¼€æ”¾æºç çš„åˆ†å¸ƒå¼åº”ç”¨ç¨‹åºåè°ƒæœåŠ¡ï¼Œæ˜¯Googleçš„Chubbyä¸€ä¸ªå¼€æºçš„å®ç°ï¼Œæ˜¯Hadoopå’ŒHbaseçš„é‡è¦ç»„ä»¶ã€‚å®ƒæ˜¯ä¸€ä¸ªä¸ºåˆ†å¸ƒå¼åº”ç”¨æä¾›ä¸€è‡´æ€§æœåŠ¡çš„è½¯ä»¶ï¼Œæä¾›çš„åŠŸèƒ½åŒ…æ‹¬ï¼šé…ç½®ç»´æŠ¤ã€åŸŸåæœåŠ¡ã€åˆ†å¸ƒå¼åŒæ­¥ã€ç»„æœåŠ¡ç­‰ã€‚
ZooKeeperçš„ç›®æ ‡å°±æ˜¯å°è£…å¥½å¤æ‚æ˜“å‡ºé”™çš„å…³é”®æœåŠ¡ï¼Œå°†ç®€å•æ˜“ç”¨çš„æ¥å£å’Œæ€§èƒ½é«˜æ•ˆã€åŠŸèƒ½ç¨³å®šçš„ç³»ç»Ÿæä¾›ç»™ç”¨æˆ·ã€‚ -----ç™¾åº¦ç™¾ç§‘

**Zookeeperæ¶æ„å›¾**
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2018111420483233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1ODMzMTY=,size_16,color_FFFFFF,t_70)
ä¸ºä»€ä¹ˆä½¿ç”¨ZKå®ç°HAå‘¢?å› ä¸ºå®˜æ–¹æ˜¯è¿™æ ·è¯´çš„ã€‚ğŸ¤”å®é™…ä¸Šä¹Ÿå¾ˆå¥½ç†è§£ã€‚ZooKeeperæä¾›äº†ä¸€ä¸ªLeader Electionæœºåˆ¶ï¼Œåˆ©ç”¨è¿™ä¸ªæœºåˆ¶èƒ½å¤Ÿä¿è¯å°½ç®¡é›†ç¾¤å­˜åœ¨å¤šä¸ªMasterå¯æ˜¯å”¯ç‹¬ä¸€ä¸ªæ˜¯Activeçš„ï¼Œå…¶å®ƒçš„éƒ½æ˜¯Standbyï¼Œå½“Activeçš„Masterå‡ºç°é—®é¢˜æ—¶ã€‚å¦å¤–çš„ä¸€ä¸ªStandby Masterä¼šè¢«é€‰ä¸¾å‡ºæ¥ã€‚å› ä¸ºé›†ç¾¤çš„ä¿¡æ¯ï¼ŒåŒ…å«Workerï¼Œ Driverå’ŒApplicationçš„ä¿¡æ¯éƒ½å·²ç»æŒä¹…åŒ–åˆ°æ–‡ä»¶ç³»ç»Ÿï¼Œå› æ­¤åœ¨åˆ‡æ¢çš„è¿‡ç¨‹ä¸­åªä¼šå½±å“æ–°Jobçš„æäº¤ã€‚å¯¹äºæ­£åœ¨è¿›è¡Œçš„Jobæ²¡æœ‰ä»€ä¹ˆå½±å“ã€‚æ‡‚äº†å§!é«˜å¯ç”¨å°±è¿™æ ·æ¥çš„ã€‚

**Zookeeper å®‰è£…é…ç½®**

1. å¤åˆ¶é…ç½®æ–‡ä»¶ï¼š

>åœ¨Zookeeperçš„ä¸»ç›®å½•çš„confæ–‡ä»¶å¤¹ä¸‹ï¼Œæœ‰ä¸€ä¸ªå®ä¾‹çš„é…ç½®æ–‡ä»¶zoo_sample.cfgï¼Œæˆ‘ä»¬å¤åˆ¶ä¸€ä»½å‡ºæ¥ï¼š

* $ cd /opt/modules/zookeeper-3.4.5/conf

* $ cp -a zoo_sample.cfg zoo.cfg

* $ vi zoo.cfg

* $ mkdir data


2. é…ç½®æ•°æ®å­˜å‚¨ç›®å½•ï¼š

>æŒ‰ç…§å¼€å‘è§„èŒƒï¼Œæˆ‘ä»¬é€šå¸¸åœ¨zookeeperçš„å®‰è£…ç›®å½•ä¸‹ï¼Œæ–°å»ºä¸€ä¸ªç›®å½•dataï¼Œå°†è¿™ä¸ªç›®å½•ä½œä¸ºzookeeperçš„æ•°æ®å­˜å‚¨ç›®å½•ã€‚

* dataDir=/opt/modules/zookeeper-3.4.5/data

* zoo.cfgé‡è¦å‚æ•°ä»‹ç»ï¼š

* tickTimeï¼šzookeeperæœåŠ¡å™¨ä¸æœåŠ¡å™¨ä¹‹é—´ï¼Œæˆ–è€…æœåŠ¡å™¨ä¸å®¢æˆ·ç«¯ä¹‹é—´ç»´æŒå¿ƒè·³çš„æ—¶é—´é—´éš”ã€‚å•ä½ï¼šæ¯«ç§’ã€‚

* clientPortï¼šå®¢æˆ·ç«¯è¿æ¥zookeeperæœåŠ¡å™¨çš„ç«¯å£ã€‚é»˜è®¤2181

* dataDirï¼šZookeeperä¿å­˜æ•°æ®çš„ç›®å½•ã€‚

3. åœ¨zoo.cfgä¸­æ·»åŠ æœåŠ¡å™¨ä¿¡æ¯ï¼š

* æ ¼å¼ï¼š server.A=B:C:D  ã€‚ åœ¨zoo.cfgä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼š

* server.1=192.168.1.205:2888:3888

* server.2=192.168.1.206:2888:3888

* server.3=192.168.1.207:2888:3888

4. åœ¨æ¯å°æœåŠ¡å™¨çš„$ZOOKEEPER_HOME/data/ç›®å½•åˆ›å»ºmyidæ–‡ä»¶ã€‚

* åœ¨masteræœåŠ¡å™¨ï¼ˆ192.168.1.205ï¼‰ä¸Šï¼Œcd åˆ°$ZOOKEEPER_HOME/data/ç›®å½•ï¼Œåˆ›å»ºmyidæ–‡ä»¶ï¼Œmyidæ–‡ä»¶çš„å†…å®¹æ˜¯1

* åœ¨slave1æœåŠ¡å™¨ï¼ˆ192.168.1.206ï¼‰ä¸Šï¼Œcd åˆ°$ZOOKEEPER_HOME/data/ç›®å½•ï¼Œåˆ›å»ºmyidæ–‡ä»¶ï¼Œmyidæ–‡ä»¶çš„å†…å®¹æ˜¯2

* åœ¨slave2æœåŠ¡å™¨ï¼ˆ192.168.1.207ï¼‰ä¸Šï¼Œcd åˆ°$ZOOKEEPER_HOME/data/ç›®å½•ï¼Œåˆ›å»ºmyidæ–‡ä»¶ï¼Œmyidæ–‡ä»¶çš„å†…å®¹æ˜¯3
* åœ¨ä¸€å°æœºå™¨ä¸Šé…ç½®å¥½ï¼Œæ‹·è´åˆ°å…¶ä»–æœåŠ¡å™¨ï¼š

* $ scp -r zookeeper-3.4.5/ slave1:/opt/modules/



5. å¯åŠ¨Zookeeperï¼š
* $ bin/zkServer.sh start
* æŸ¥çœ‹çŠ¶æ€
* bin/zkServer.sh status

æ³¨æ„ï¼šå¯åŠ¨æ“ä½œéœ€è¦åœ¨ä¸‰å°æœåŠ¡å™¨ä¸Šéƒ½æ‰§è¡Œã€‚

**HDFS HAçš„èƒŒæ™¯**
* HDFSé›†ç¾¤ä¸­NameNode å­˜åœ¨å•ç‚¹æ•…éšœï¼ˆSPOFï¼‰ã€‚å¯¹äºåªæœ‰ä¸€ä¸ªNameNodeçš„é›†ç¾¤ï¼Œå¦‚æœNameNodeæœºå™¨å‡ºç°æ„å¤–æƒ…å†µï¼Œå°†å¯¼è‡´æ•´ä¸ªé›†ç¾¤æ— æ³•ä½¿ç”¨ï¼Œç›´åˆ°NameNode é‡æ–°å¯åŠ¨ã€‚

* å½±å“HDFSé›†ç¾¤ä¸å¯ç”¨ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹ä¸¤ç§æƒ…å†µï¼šä¸€æ˜¯NameNodeæœºå™¨å®•æœºï¼Œå°†å¯¼è‡´é›†ç¾¤ä¸å¯ç”¨ï¼Œé‡å¯NameNodeä¹‹åæ‰å¯ä½¿ç”¨ï¼›äºŒæ˜¯è®¡åˆ’å†…çš„NameNodeèŠ‚ç‚¹è½¯ä»¶æˆ–ç¡¬ä»¶å‡çº§ï¼Œå¯¼è‡´é›†ç¾¤åœ¨çŸ­æ—¶é—´å†…ä¸å¯ç”¨ã€‚

* ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒHadoopç»™å‡ºäº†HDFSçš„é«˜å¯ç”¨HAæ–¹æ¡ˆï¼šHDFSé€šå¸¸ç”±ä¸¤ä¸ªNameNodeç»„æˆï¼Œä¸€ä¸ªå¤„äºactiveçŠ¶æ€ï¼Œå¦ä¸€ä¸ªå¤„äºstandbyçŠ¶æ€ã€‚Active NameNodeå¯¹å¤–æä¾›æœåŠ¡ï¼Œæ¯”å¦‚å¤„ç†æ¥è‡ªå®¢æˆ·ç«¯çš„RPCè¯·æ±‚ï¼Œè€ŒStandby NameNodeåˆ™ä¸å¯¹å¤–æä¾›æœåŠ¡ï¼Œä»…åŒæ­¥Active NameNodeçš„çŠ¶æ€ï¼Œä»¥ä¾¿èƒ½å¤Ÿåœ¨å®ƒå¤±è´¥æ—¶å¿«é€Ÿè¿›è¡Œåˆ‡æ¢ã€‚

**HDFS HAçš„æ¶æ„**

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20181114210002921.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1ODMzMTY=,size_16,color_FFFFFF,t_70)
å¯¹è¿™å¼ å›¾åšä¸ªè¯´æ˜ï¼š
1. ZKFCæ§åˆ¶ç€ä¸¤ä¸ªNamenodeï¼Œå½“å…¶ä¸­ä¸€ä¸ªæŒ‚æ‰åZké€šè¿‡é€‰ä¸¾ï¼Œè¿…é€Ÿçš„å¼€å¯å¦ä¸€ä¸ªNamenodeè¿›å…¥activeçŠ¶æ€ã€‚
2. ä¸¤ä¸ªNNæ—¶åˆ»ä¿æŒæ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®çš„åŒæ­¥å’Œä¸€è‡´ã€‚
3. Datanodeæ—¶åˆ»å‘ä¸¤ä¸ªNNå‘é€reportã€‚

**HDFS HAè®¾è®¡**
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20181114211046233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1ODMzMTY=,size_16,color_FFFFFF,t_70)

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20181114212936751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1ODMzMTY=,size_16,color_FFFFFF,t_70)
**HDFS HAé…ç½®**

é…ç½®HAè¦ç‚¹ 

* share edits
	JournalNode ï¼ˆæ—¥å¿—èŠ‚ç‚¹ï¼‰è‡³å°‘ä¸‰ä¸ª
* NameNode
	Activeï¼ŒStandby
* Client
	Proxy
* fence 
	
		åŒä¸€æ—¶åˆ»ä»…ä»…æœ‰ä¸€ä¸ªNameNodeå¯¹å¤–æä¾›æœåŠ¡
		ä½¿ç”¨çš„æ–¹å¼sshfence
			ä¸¤ä¸ªNameNodeä¹‹é—´èƒ½å¤Ÿsshæ— å¯†ç ç™»å½•
			205(NameNode) ssh -> 206
			206(NameNode) ssh -> 205

è§„åˆ’é›†ç¾¤ 

|  192.168.1.205|192.168.1.206  |192.168.1.207 |
|--|--|--|
|  master|  slave1|slave2 |
| NameNode |NameNode  | |
| JournalNode| JournalNode |JournalNode |
| DataNode| DataNode |DataNode |



é…ç½®HAåä¸éœ€è¦secondarynamenodeäº†ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿé…ç½®HAåå¤‡ä»½çš„NamenodeåŸºæœ¬ä¸Šæ›¿ä»£äº†å®ƒã€‚

é…ç½®ç»†èŠ‚ [éƒ½æ˜¯å®˜ç½‘ä¸Šçš„ä¸œè¥¿](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html)

       hdfs-site.xml 
       
		<property>
		  <name>dfs.nameservices</name>
		  <value>mycluster</value>
		</property>

          // å‘½åç©ºé—´

		<property>
		  <name>dfs.ha.namenodes.mycluster</name>
		  <value>nn1,nn2</value>
		</property>

        // ç®¡ç†å“ªä¸¤ä¸ªnn

		<property>
		  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
		  <value>machine1.example.com:8020</value>
		</property>
		<property>
		  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
		  <value>machine2.example.com:8020</value>
		</property>


        // nnåœ¨å“ªå°æœºå™¨

         // é…ç½®ä¸¤ä¸ªnnè®¿é—®ç«¯å£Web
			<property>
			  <name>dfs.namenode.http-address.mycluster.nn1</name>
			  <value>machine1.example.com:50070</value>
			</property>
			<property>
			  <name>dfs.namenode.http-address.mycluster.nn2</name>
			  <value>machine2.example.com:50070</value>
			</property>


           // å…±äº«æ—¥å¿—çš„ç›®å½•
			<property>
			  <name>dfs.namenode.shared.edits.dir</name>
			  <value>qjournal://node1.example.com:8485;node2.example.com:8485;node3.example.com:8485/mycluster</value>
			</property> 

           // é…ç½®å®¢æˆ·ç«¯è®¿é—®
			<property>
			  <name>dfs.client.failover.proxy.provider.mycluster</name>
			  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
			</property>

           //é…ç½®éš”ç¦»æ–¹å¼ sshéš”ç¦»

		    <property>
		      <name>dfs.ha.fencing.methods</name>
		      <value>sshfence</value>
		    </property>
		
		    <property>
		      <name>dfs.ha.fencing.ssh.private-key-files</name>
		      <value>/home/exampleuser/.ssh/id_rsa</value>
		    </property> 

        ä½¿ç”¨çš„æ–¹å¼sshfence
		ä¸¤ä¸ªNameNodeä¹‹é—´èƒ½å¤Ÿsshæ— å¯†ç ç™»å½•
		131(NameNode) ssh -> 132
		132(NameNode) ssh -> 131 

         // é…ç½®æ—¥å¿—æ–‡ä»¶å†™åœ¨æœ¬åœ°é‚£ä¸ªç›®å½•
			<property>
			  <name>dfs.journalnode.edits.dir</name>
			  <value>
                 /opt/app/hadoop-2.7.3/data/dfs/jn
               </value>
			</property>

			//é…ç½® core-site.xml
			
			 <property>
			  <name>fs.defaultFS</name>
			  <value>hdfs://mycluster</value>
			</property>

æœ€åå‘å…¶ä»–ä¸¤ä¸ªèŠ‚ç‚¹åŒæ­¥æ–‡ä»¶

å¯åŠ¨é¡ºåº

* ./hadoop-daemon.sh start journalnodeï¼ˆæ¯ä¸ªï¼‰
* bin/hdfs namenode -formatï¼ˆnn1ï¼‰
* sbin/hadoop-daemon.sh start namenode
* ./hdfs namenode -bootstrapStandbyï¼ˆnn2ï¼‰
*  ./hadoop-daemon.sh start datanode
*  ./hdfs haadmin -transitionToActive nn1

 ~~åƒä¸‡ä¸è¦æ‰‹è´±ä¹±æ ¼å¼åŒ–ï¼Œè´¼çƒ¦ï¼~~ 

 **ZKå®ç°è‡ªåŠ¨æ•…éšœè½¬æ¢**
 ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20181114213002765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1ODMzMTY=,size_16,color_FFFFFF,t_70)
HA è‡ªåŠ¨æ•…éšœè½¬ç§»   Zookeeper

|  192.168.1.205|192.168.1.206  |192.168.1.207 |
|--|--|--|
|  master|  slave1|slave2 |
| NameNode |NameNode  | |
| ZKFC |ZKFC  | |
| JournalNode| JournalNode |JournalNode |
| DataNode| DataNode |DataNode |
å¯åŠ¨åéƒ½æ˜¯Standbyï¼Œéœ€è¦é€‰ä¸¾ä¸€ä¸ªä¸ºActiveã€‚ZKFCå®ç°å¯¹ä¸¤ä¸ªNamenodeçš„å®æ—¶ç›‘æ§ã€‚

é…ç½®ç»†èŠ‚ï¼š
* hdfs-site.xml 

          // å¯åŠ¨æ•…éšœè‡ªåŠ¨è½¬ç§»
		 <property>
		   <name>dfs.ha.automatic-failover.enabled</name>
		   <value>true</value>
		 </property>

        // ä¾èµ–ZK  éœ€è¦çŸ¥é“ZKåœ¨å“ª
* core-site.xml

		<property>
		   <name>ha.zookeeper.quorum</name>
		   <value>master:2181,slave1:2181,slave2:2181</value>
		 </property>
å¯åŠ¨ï¼š
* å…³é—­æ‰€æœ‰çš„HDFSæœåŠ¡ï¼šstop-dfs.sh
* å¯åŠ¨zookeeperé›†ç¾¤ï¼šzkServer.sh start
* åˆå§‹åŒ–ZKä¸­çš„çŠ¶æ€ï¼šhdfs zkfc -formatZK
* å¯åŠ¨HDFSæœåŠ¡ï¼šstart-dfs.sh

è¿™é‡Œçš„start-dfs.sh å¯åŠ¨ä¸€åˆ‡  åŒ…æ‹¬zkFcã€‚

![](https://i.imgur.com/DDrWG02.png)

å€¼å¾—ä¸€æçš„æ˜¯zkæŒ‚æ‰ä¸ä¼šå½±å“é›†ç¾¤ï¼Œåªæ˜¯ä¸èƒ½æ•…éšœè½¬ç§»äº†ï¼Œå¤±å»äº†å¯¹é›†ç¾¤çš„é€‰ä¸¾ã€‚æœ€å~~å°å£°è¯´ä¸€è¯´HAå¤ªç®€å•äº†ï¼~~ ğŸ˜‡ğŸ˜‡ğŸ˜‡

## Federationè”ç›Ÿ
---
NameNode
	
	èƒ½ä¸èƒ½æœ‰å¤šä¸ªNameNode
	NameNode 			NameNode 				NameNode
	å…ƒæ•°æ®				å…ƒæ•°æ®					å…ƒæ•°æ®
	log				machine					ç”µå•†æ•°æ®/è¯å•æ•°æ®

ä¸‰ä¸ªæ¨¡å—æŠŠæ•°æ®å­˜åœ¨æ‰€æœ‰çš„DataNodesèŠ‚ç‚¹ä¸Šï¼Œäºæ˜¯å‡ºç°äº†ä¸‹é¢çš„æ¶æ„ï¼Œæä¾›è§£å†³æ–¹æ¡ˆã€‚

![](https://i.imgur.com/mkONCnL.png)



## é›†ä¸­å¼ç¼“å­˜ç®¡ç† 
---
>HDFSå…è®¸ç”¨æˆ·å°†ä¸€éƒ¨åˆ†æ–‡ä»¶æˆ–ç›®å½•ç¼“å­˜åœ¨HDFSä¸­ï¼ŒNamenodeä¼šé€šçŸ¥æ‹¥æœ‰å¯¹åº”å¿«çš„DataNodeså°†å…¶ç¼“å­˜åœ¨datanodeçš„å†…å­˜ä¸­ã€‚

## åˆ†å¸ƒå¼æ‹·è´
---

>æ•°æ®è¿ç§»ï¼Œå¦‚å°†æµ‹è¯•é›†ç¾¤çš„æ•°æ®æ‹·è´åˆ°ç”Ÿäº§é›†ç¾¤ è¯¦è§å®˜ç½‘ [ç‚¹æˆ‘](http://hadoop.apache.org/docs/stable/hadoop-distcp/DistCp.html) 

æ‹·è´å‘½ä»¤ï¼š 

 hadoop distcp -i hftp://sourceFS:50070/src hdfs://destFS:8020/dest

## Yarnçš„HA
---
>åœ¨ç†Ÿç»ƒé…ç½®äº†HDFSçš„HAåï¼ŒYarnçš„HAé…ç½®ä¹Ÿæ˜¯ååˆ†ç®€å•çš„ï¼Œå®ƒçš„æ¶æ„åŸºæœ¬ä¸Šå’ŒHDFSçš„æ˜¯ä¸€æ ·çš„ã€‚ä¹Ÿæ˜¯é€šè¿‡zké€‰ä¸¾RMæ¥å®ç°é«˜å¯ç”¨ã€‚

![](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/images/rm-ha-overview.png) 


è¯¦ç»†é…ç½® [éƒ½æ˜¯å®˜ç½‘ä¸Šçš„ä¸œè¥¿](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html)

		<property>
		  <name>yarn.resourcemanager.ha.enabled</name>
		  <value>true</value>
		</property>
		<property>
		  <name>yarn.resourcemanager.cluster-id</name>
		  <value>cluster1</value>
		</property>
		<property>
		  <name>yarn.resourcemanager.ha.rm-ids</name>
		  <value>rm1,rm2</value>
		</property>
		<property>
		  <name>yarn.resourcemanager.hostname.rm1</name>
		  <value>master1</value>
		</property>
		<property>
		  <name>yarn.resourcemanager.hostname.rm2</name>
		  <value>master2</value>
		</property>
		<property>
		  <name>yarn.resourcemanager.webapp.address.rm1</name>
		  <value>master1:8088</value>
		</property>
		<property>
		  <name>yarn.resourcemanager.webapp.address.rm2</name>
		  <value>master2:8088</value>
		</property>
		<property>
		  <name>yarn.resourcemanager.zk-address</name>
		  <value>zk1:2181,zk2:2181,zk3:2181</value>
		</property>

å‘½ä»¤è¯¦è§å®˜ç½‘ [ç‚¹æˆ‘](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html)


		 $ yarn rmadmin -getServiceState rm1
		 active
		
		 $ yarn rmadmin -getServiceState rm2
		 standby



		 $ yarn rmadmin -transitionToStandby rm1
		 Automatic failover is enabled for org.apache.hadoop.yarn.client.RMHAServiceTarget@1d8299fd
		 Refusing to manually manage HA state, since it may cause
		 a split-brain scenario or other incorrect state.
		 If you are very sure you know what you are doing, please
		 specify the forcemanual flag.

## æ€»ç»“
----
HAè¿™ä¸€å—ï¼Œä½ è¦æ˜¯ç…§ç€å®˜æ–¹æ–‡æ¡£é…ç½®å…¶å®è¿˜æ˜¯æŒºç®€å•çš„ï¼Œä¸€å¹´å‰çš„æˆ‘ç›´æ¥è·³è¿‡äº†HAï¼ŒåªçŸ¥é“æ˜¯é«˜å¯ç”¨ã€‚æƒ³æƒ³è¿˜æ˜¯å¤ªå¹´è½»äº†ã€‚åœ¨åº”ç”¨è¿™ä¸€å—ï¼Œé…ç½®å¥½èƒ½æ”¾åˆ°ç”Ÿäº§ç¯å¢ƒåŸºæœ¬ä¸Šå°±å¯ä»¥äº†å§ï¼åˆ°æ­¤å‘¢Hadoopçš„å››ä¸ªæ ¸å¿ƒæ¨¡å—çš„å†…å®¹å°±å‘Šä¸€æ®µè½äº†ï¼ŒèŠ±äº†å››å¤©æ—¶é—´è¾¹å­¦è¾¹ç»ƒã€‚ä»æ—©åšåˆ°æ™šçš„å­¦ä¹ æˆ‘å°½ç„¶ä¸å›°ğŸ˜ªã€‚å¯èƒ½è¿™å°±æ˜¯çŸ¥è¯†çš„åŠ›é‡å§ï¼åˆçœ‹ä¹¦åˆçœ‹è§†é¢‘è¿˜çœ‹å®˜æ–¹æ–‡æ¡£ï¼Œå°½ç„¶ä¸è„±å‘ï¼Ÿå°‘å¹´ä½ æ¸´æœ›åŠ›é‡å—ï¼ŸğŸ‘·ä¸‹é¢çš„å°±æ˜¯Hiveçš„æ¸©ä¹ äº†ï¼ŒHiveä¸è®ºå¯¹è°æ¥è¯´åº”è¯¥éƒ½æ¯”MRç®€å•å§ï¼Œæˆ‘åº”è¯¥èƒ½å¾ˆå¿«çš„è¿‡è¿‡å»ï¼ŒåŠ ç´§æ—¶é—´å­¦ä¹ ï¼åé¢è¿˜æœ‰ä¸€å¤§ç‰‡å‘¢ï¼